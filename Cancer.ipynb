{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd  # Version 1.3.5\n",
    "import numpy as np  # Version 1.21.4\n",
    "import matplotlib.pyplot as plt  # Version 3.5.0\n",
    "import seaborn as sns  # Version 0.11.2\n",
    "from sklearn.model_selection import train_test_split  # Version 0.24.2\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import tensorflow as tf  # Version 2.7.0\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from ucimlrepo import fetch_ucirepo  # Ensure the latest version is installed\n",
    "\n",
    "# Python version: 3.9\n",
    "\n",
    "# Step 1: Fetch dataset\n",
    "# Using the UCI Machine Learning Repository's API to fetch the Breast Cancer Wisconsin Diagnostic dataset\n",
    "breast_cancer_wisconsin_diagnostic = fetch_ucirepo(id=17)\n",
    "\n",
    "# Extract features (X) and target (y) from the dataset\n",
    "X = breast_cancer_wisconsin_diagnostic.data.features  # Features are numerical values of tumor measurements\n",
    "y = breast_cancer_wisconsin_diagnostic.data.targets  # Targets represent tumor diagnosis (Malignant or Benign)\n",
    "\n",
    "# Display dataset metadata (optional, for reference only)\n",
    "print(\"Dataset Metadata:\")\n",
    "print(breast_cancer_wisconsin_diagnostic.metadata)\n",
    "\n",
    "# Step 2: Preprocess the data\n",
    "# Normalize features using StandardScaler to mean=0 and variance=1\n",
    "# This helps in improving the performance of the model by standardizing feature values\n",
    "standard_scaler = StandardScaler()\n",
    "X = standard_scaler.fit_transform(X)\n",
    "\n",
    "# Encode labels: 0 for benign, 1 for malignant\n",
    "# Label encoding converts the target labels to a numerical format suitable for the model\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# Step 3: Split data into training and validation sets\n",
    "# Splitting ensures we have separate data for training the model and validating its performance\n",
    "# 80% of the data is used for training, and 20% for validation/testing\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Step 4: Define the model\n",
    "# Building a simple neural network with input, hidden, and output layers\n",
    "# The architecture includes one input layer, one hidden layer, and one output layer\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(units=16, activation='relu', input_shape=(X.shape[1],)),  # Input layer with 16 neurons\n",
    "    tf.keras.layers.Dense(units=32, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),  # Hidden layer with 32 neurons\n",
    "    tf.keras.layers.Dropout(0.3),  # Dropout layer to prevent overfitting during training\n",
    "    tf.keras.layers.Dense(units=1, activation='sigmoid')  # Output layer with sigmoid activation for binary classification\n",
    "])\n",
    "\n",
    "# Step 5: Compile the model\n",
    "# Compiling configures the model for training by defining the optimizer, loss function, and evaluation metrics\n",
    "# Adam optimizer is used for adaptive learning rate, and binary crossentropy is suited for binary classification tasks\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary for reference\n",
    "# This provides an overview of the model architecture and total trainable parameters\n",
    "model.summary()\n",
    "\n",
    "# Step 6: Train the model\n",
    "# Early stopping callback to prevent overfitting\n",
    "# Training stops early if validation loss does not improve for 2 consecutive epochs\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "\n",
    "# Train the model with validation data\n",
    "# Validation data is used during training to monitor the model's performance on unseen data\n",
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=50,  # Training for up to 50 epochs (stops earlier if early stopping is triggered)\n",
    "    validation_data=(x_test, y_test),\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1  # Verbose output to track training progress\n",
    ")\n",
    "\n",
    "# Step 7: Evaluate the model\n",
    "# Test performance on validation data\n",
    "# Evaluates the model on the test set and prints the loss and accuracy\n",
    "test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"\\nTest Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Step 8: Make predictions\n",
    "# Predict probabilities for test data using the trained model\n",
    "predictions = model.predict(x_test)\n",
    "\n",
    "# Convert probabilities to binary predictions (threshold = 0.5)\n",
    "# Predictions above 0.5 are classified as malignant, and below 0.5 as benign\n",
    "predicted_classes = (predictions > 0.5).astype(int)\n",
    "\n",
    "# Step 9: Visualize Results\n",
    "# Confusion Matrix\n",
    "# The confusion matrix provides insights into the number of true positives, false positives, true negatives, and false negatives\n",
    "cm = confusion_matrix(y_test, predicted_classes)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# Classification Report\n",
    "# The classification report shows precision, recall, F1-score, and support for each class\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, predicted_classes, target_names=label_encoder.classes_))\n",
    "\n",
    "# Training and Validation Loss\n",
    "# Plotting the training and validation loss over epochs helps visualize convergence and overfitting\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Training and Validation Accuracy\n",
    "# Plotting the training and validation accuracy over epochs helps visualize model performance\n",
    "plt.figure()\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Accuracy Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Step 10: Save the model\n",
    "# Save the trained model for reuse\n",
    "# The model is saved in HDF5 format, which can be loaded later for predictions or further training\n",
    "model.save('breast_cancer_classification_model.h5')\n",
    "print(\"Model saved as 'breast_cancer_classification_model.h5'\")\n",
    "\n",
    "# Save the scaler and label encoder for preprocessing new data\n",
    "# The scaler and label encoder are saved using joblib for consistency during deployment\n",
    "import joblib\n",
    "joblib.dump(standard_scaler, 'scaler.pkl')\n",
    "joblib.dump(label_encoder, 'label_encoder.pkl')\n",
    "print(\"Scaler and Label Encoder saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
